{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import _use_shared_memory\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "path=\"\"\n",
    "trainX = np.load(path + 'train.npy', encoding='bytes')\n",
    "trainY = np.load(path + 'train_transcripts.npy', encoding='bytes')\n",
    "\n",
    "valX = np.load(path + 'dev.npy', encoding='bytes')\n",
    "valY = np.load(path + 'dev_transcripts.npy', encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_charmap(corpus):\n",
    "    chars = list(set(corpus))\n",
    "    chars.sort()\n",
    "    charmap = {c: i for i, c in enumerate(chars)}\n",
    "    return chars, charmap\n",
    "\n",
    "\n",
    "def map_corpus(corpus, charmap):\n",
    "    return torch.IntTensor([int(charmap[c]) for c in corpus])\n",
    "\n",
    "\n",
    "corpus = \" \".join(trainY)\n",
    "chars, charmap = get_charmap(corpus)\n",
    "charcount = len(chars)\n",
    "print(\"Unique character count: {}\".format(len(chars)))\n",
    "array = map_corpus(corpus, charmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chars)\n",
    "print(charmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, x, y, evaluate=False):\n",
    "        self.x = [torch.from_numpy(utterance).float() for utterance in x]\n",
    "        self.y = [map_corpus(utterance, charmap) for utterance in y]\n",
    "        \n",
    "        self.evaluate = evaluate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.evaluate is True:\n",
    "            return self.x[ idx ], torch.Tensor(1)\n",
    "        else:\n",
    "            return self.x[ idx ], self.y[ idx ]\n",
    "\n",
    "valSet =  SpeechDataset(x=valX, y=valY)\n",
    "trainSet = SpeechDataset(x=trainX, y=trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "attention_dim = 128\n",
    "hidden_dim = 256\n",
    "SOS = charcount\n",
    "EOS = charcount + 1\n",
    "\n",
    "\n",
    "class SequencePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequencePooling, self).__init__()\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, lengths = pad_packed_sequence(h)\n",
    "        max_length = h.size(0)\n",
    "        new_length = max_length //2\n",
    "        \n",
    "        h = h.transpose(0, 1)\n",
    " \n",
    "        # pooling\n",
    "        if max_length % 2 == 1:\n",
    "            h = h[:, 0:max_length - 1 , :]\n",
    "        h = h.contiguous().view(batch_size, new_length, 2, hidden_dim*2)\n",
    "        h = torch.mean(h, 2)        \n",
    "        h = h.transpose(0, 1)\n",
    " \n",
    "        return pack_padded_sequence(h, [l // 2 for l in lengths], batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=40, hidden_size=hidden_dim, batch_first=False, bidirectional=True)\n",
    "        \n",
    "        self.pbilstm1 = nn.LSTM(input_size=hidden_dim*2, hidden_size=hidden_dim, batch_first=False, bidirectional=True)\n",
    "        self.pbilstm2 = nn.LSTM(input_size=hidden_dim*2, hidden_size=hidden_dim, batch_first=False, bidirectional=True)\n",
    "        self.pbilstm3 = nn.LSTM(input_size=hidden_dim*2, hidden_size=hidden_dim, batch_first=False, bidirectional=True)\n",
    "        \n",
    "        self.sequencepooling = SequencePooling()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features=hidden_dim*2, out_features=attention_dim)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_dim*2, out_features=attention_dim)\n",
    "        \n",
    "    def forward(self, utterance_inputs, lengths):\n",
    "        h = utterance_inputs\n",
    "\n",
    "        h = pack_padded_sequence(h, lengths.data.cpu().numpy(), batch_first=False)         \n",
    "        \n",
    "        h, _ = self.bilstm(h)\n",
    "        \n",
    "        h = self.sequencepooling(h)\n",
    "        \n",
    "        h, _ =  self.pbilstm1(h)\n",
    "        \n",
    "        h = self.sequencepooling(h)\n",
    "        h, _ =  self.pbilstm2(h)\n",
    "        \n",
    "        h = self.sequencepooling(h)\n",
    "        h, _ =  self.pbilstm3(h)\n",
    "        \n",
    "        h, _ = pad_packed_sequence(h)\n",
    "              \n",
    "        attention_keys = self.linear1(h)\n",
    "        attention_values = self.linear2(h)\n",
    "        \n",
    "                \n",
    "        if attention_keys.data.sum() !=attention_keys.data.sum():\n",
    "            pdb.set_trace()\n",
    "        \n",
    "        return attention_keys, attention_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=charcount+2, embedding_dim=attention_dim)\n",
    "\n",
    "        self.h_0_1 = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        self.c_0_1 = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        self.h_0_2 = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        self.c_0_2 = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        self.h_0_3 = nn.Parameter(torch.zeros(1, attention_dim))\n",
    "        self.c_0_3 = nn.Parameter(torch.zeros(1, attention_dim))\n",
    "        \n",
    "        self.h1 = self.h_0_1.expand(batch_size,-1)\n",
    "        self.h2 = self.h_0_2.expand(batch_size,-1)\n",
    "        self.h3 = self.h_0_3.expand(batch_size,-1)\n",
    "        \n",
    "        self.c1 = self.c_0_1.expand(batch_size,-1)\n",
    "        self.c2 = self.c_0_2.expand(batch_size,-1)\n",
    "        self.c3 = self.c_0_3.expand(batch_size,-1)\n",
    "        \n",
    "        \n",
    "        self.cell = nn.LSTMCell(input_size=attention_dim*2, hidden_size=hidden_dim)\n",
    "        self.cell2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n",
    "        self.cell3 = nn.LSTMCell(input_size=hidden_dim, hidden_size=attention_dim)\n",
    "         \n",
    "    def forward(self, inputs, previous_context, char_index):\n",
    "        h = inputs\n",
    "        \n",
    "        h = self.embedding(h)\n",
    "        \n",
    "        previous_context = previous_context.squeeze()\n",
    "\n",
    "        h = torch.cat((h,previous_context), 1)\n",
    "        \n",
    "        \n",
    "        if char_index == 0:\n",
    "            self.h1 = self.h_0_1.expand(batch_size,-1)\n",
    "            self.h2 = self.h_0_2.expand(batch_size,-1)\n",
    "            self.h3 = self.h_0_3.expand(batch_size,-1)\n",
    "\n",
    "            self.c1 = self.c_0_1.expand(batch_size,-1)\n",
    "            self.c2 = self.c_0_2.expand(batch_size,-1)\n",
    "            self.c3 = self.c_0_3.expand(batch_size,-1)\n",
    "        \n",
    "        self.h1, self.c1 = self.cell(h, (self.h1, self.c1))\n",
    "        \n",
    "        self.h2, self.c2 = self.cell2(self.h1, (self.h2, self.c2))\n",
    "        \n",
    "        self.h3, self.c3 = self.cell3(self.h2, (self.h3, self.c3))\n",
    "        \n",
    "        return self.h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LasModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LasModel, self).__init__()\n",
    "        \n",
    "        self.encoder = TextEncoder()\n",
    "        self.decoder = TextDecoder()\n",
    "\n",
    "        self.linearMlp = nn.Linear(in_features=hidden_dim, out_features=attention_dim)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=attention_dim, out_features=charcount+2)\n",
    "        \n",
    "    def forward(self, utterance_inputs, lengths, transcript_inputs, transcript_targets, transcript_lengths):\n",
    "        attention_keys, attention_values = self.encoder(utterance_inputs, lengths)\n",
    "\n",
    "        context = Variable(torch.FloatTensor(batch_size, 1, attention_dim).zero_()).cuda()\n",
    "\n",
    "        max_utterance_length = utterance_inputs.size(0)\n",
    "        max_utterance_length_after_pbilstm = max_utterance_length//8\n",
    "        \n",
    "        max_transcript_length = transcript_inputs.size(1)\n",
    "        \n",
    "        attention_keys = attention_keys.transpose(0,1).transpose(1,2)\n",
    "\n",
    "        attention_values = attention_values.transpose(0,1)\n",
    "        \n",
    "        mask = torch.FloatTensor(batch_size, 1, max_utterance_length_after_pbilstm).zero_()\n",
    "        for i in range(batch_size):\n",
    "            utterance_length = int(lengths[i])//8\n",
    "            mask[ i, 0,  0:utterance_length] = torch.ones(1, utterance_length).float()\n",
    "        mask = Variable(mask).cuda()\n",
    "\n",
    "        out = []\n",
    "        for i in range(max_transcript_length):\n",
    "            chars = transcript_inputs[:, i]  #32 x 1  teacher forcing all the time!\n",
    "\n",
    "            query = self.decoder(chars, context, i)\n",
    "\n",
    "            energy = torch.bmm(query.unsqueeze(1), attention_keys)\n",
    "\n",
    "            attention = F.softmax(energy, dim=2)\n",
    "\n",
    "            attention = attention * mask\n",
    "\n",
    "            attention = attention/torch.sum(attention, 2).unsqueeze(2)\n",
    "\n",
    "            context = torch.bmm(attention, attention_values)\n",
    "\n",
    "            context = context.squeeze(1)\n",
    "\n",
    "            mlp_input = torch.cat((context,query), 1)\n",
    "\n",
    "            logit = self.linearMlp( mlp_input )\n",
    "            logit = self.lrelu(logit)\n",
    "            logit = self.linear(logit)\n",
    "\n",
    "            out += [logit]\n",
    "\n",
    "        out = torch.stack(out,1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "        \n",
    "    lengths = [utterance[0].size(0) for utterance in batch]\n",
    "\n",
    "    lengths = torch.IntTensor(lengths)\n",
    "    \n",
    "    lengths, length_idx = lengths.sort(0, descending=True)\n",
    "\n",
    "    max_utterance_length = lengths[0]\n",
    "    \n",
    "    transcript_lengths = [utterance[1].size(0) for utterance in batch]\n",
    "    \n",
    "    max_transcript_length = max(transcript_lengths)+1\n",
    "    \n",
    "    if _use_shared_memory:\n",
    "        utterance_inputs = torch.FloatStorage.storage()._new_shared(max_utterance_length*batch_size*40).new(batch_size, 40, max_utterance_length).zero_()\n",
    "        transcript_inputs = torch.LongStorage.storage()._new_shared(batch_size*max_transcript_length).new(batch_size, max_transcript_length).zero_()\n",
    "        transcript_targets = torch.LongStorage.storage()._new_shared(batch_size*max_transcript_length).new(batch_size, max_transcript_length).zero_()\n",
    "        transcript_lengths = torch.IntStorage.storage()._new_shared(batch_size).new(batch_size,).zero_()\n",
    "    else:\n",
    "        utterance_inputs = torch.FloatTensor(batch_size, 40, max_utterance_length).zero_()\n",
    "        transcript_inputs = torch.LongTensor(batch_size, max_transcript_length).zero_()\n",
    "        transcript_targets = torch.LongTensor(batch_size, max_transcript_length).zero_()\n",
    "        transcript_lengths = torch.IntTensor(batch_size,).zero_()\n",
    "\n",
    "    for idx, sorted_idx in enumerate(length_idx):\n",
    "        utterance_input = batch[sorted_idx][0] #length x 40\n",
    "        utterance_inputs[idx, :, 0:utterance_input.size(0)] = torch.transpose(utterance_input, 0 , 1)\n",
    "        \n",
    "        mapped_transcript = batch[sorted_idx][1]\n",
    "        transcript_lengths[idx] = mapped_transcript.size(0) + 1\n",
    "        \n",
    "        transcript_inputs[idx, 1:mapped_transcript.size(0)+1] = mapped_transcript\n",
    "        transcript_inputs[idx, 0] = SOS\n",
    "        \n",
    "        transcript_targets[idx, 0:mapped_transcript.size(0)] = mapped_transcript\n",
    "        transcript_targets[idx, mapped_transcript.size(0)] = EOS\n",
    "    \n",
    "    utterance_inputs = utterance_inputs.transpose(0,2).transpose(1,2)\n",
    "    \n",
    "    return utterance_inputs, lengths, transcript_inputs, transcript_targets, transcript_lengths\n",
    "\n",
    "pin_memory = False \n",
    "if use_cuda:\n",
    "    pin_memory = True \n",
    "print(\"using cuda: \", use_cuda)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainSet, num_workers=2, pin_memory=pin_memory, shuffle=True,\n",
    "                                           batch_size=batch_size, collate_fn=my_collate)\n",
    "val_loader = torch.utils.data.DataLoader(valSet,  num_workers=2, pin_memory=pin_memory,\n",
    "                                           batch_size=batch_size, collate_fn=my_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss3D(nn.CrossEntropyLoss):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss3D, self).__init__(reduce=False)\n",
    "    def forward(self, input, target):\n",
    "        return super(CrossEntropyLoss3D, self).forward(input.view(-1, input.size()[2]), target.view(-1))\n",
    "\n",
    "model = LasModel()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_function = CrossEntropyLoss3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "model.train()\n",
    "\n",
    "#model.load_state_dict(torch.load('model_epoch_84.pt'))\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    for batch_index, sample in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        utterance_inputs = Variable(sample[0]).cuda()\n",
    "        lengths = Variable(sample[1]).cuda()\n",
    "        transcript_inputs = Variable(sample[2]).cuda()\n",
    "        transcript_targets = Variable(sample[3]).cuda()\n",
    "        transcript_lengths = Variable(sample[4]).cuda()\n",
    "\n",
    "        output = model(utterance_inputs, lengths, transcript_inputs, transcript_targets, transcript_lengths)\n",
    "\n",
    "        loss = loss_function(output, transcript_targets)  \n",
    "                        \n",
    "        if loss.data.sum() !=loss.data.sum():\n",
    "            pdb.set_trace()\n",
    "\n",
    "        maxlength = transcript_targets.size(1)\n",
    "        transcript_mask = torch.FloatTensor(batch_size, maxlength).zero_()\n",
    "        for i in range(batch_size):\n",
    "            transcript_length = int(transcript_lengths[i])\n",
    "            transcript_mask[ i, 0:transcript_length] = torch.ones(1, transcript_length).float()\n",
    "        transcript_mask = Variable(transcript_mask).cuda()\n",
    "\n",
    "        loss = loss.view(batch_size, maxlength)\n",
    "\n",
    "        loss = loss * transcript_mask\n",
    "\n",
    "        loss = torch.sum(loss, dim=1)\n",
    "\n",
    "        \n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.sum()\n",
    "       \n",
    "        print(\"batch_index\", batch_index, \"loss batch: \", loss.data.sum(), \", epoch loss: \", epoch_loss/(batch_index+1))\n",
    "\n",
    "    print(\"loss epoch: \", epoch_loss/len(train_loader))\n",
    "    torch.save(model.state_dict(), \"model_epoch_\"+str(epoch)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_epoch_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
